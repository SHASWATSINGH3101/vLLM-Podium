{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installation","metadata":{"_uuid":"0525e8b2-4ec3-4020-bc4f-28ece042488b","_cell_guid":"07a9f715-c706-4cde-8ad0-5d8b000cb181","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# \n# !pip install huggingface_hub","metadata":{"_uuid":"114dab51-ddd3-411a-9147-cb6345c58b4b","_cell_guid":"d435bfd7-66b3-4be7-a480-f33cc5bc0004","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# \n# !pip install --upgrade transformers","metadata":{"_uuid":"5f40a8ed-c933-4320-9dd7-f7a41effc05e","_cell_guid":"c7085c50-31bd-48ae-b958-64b1adf8762b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#\n# from huggingface_hub import notebook_login\n# notebook_login()","metadata":{"_uuid":"398feb2c-3c1c-4308-b32c-ff74db81b381","_cell_guid":"40d34048-e656-47aa-8abe-3c61f41aa218","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# \n!pip install -U vllm torch","metadata":{"_uuid":"f7a8eb3e-ce1a-45c2-a985-5ebc57c781ed","_cell_guid":"0151ada7-8ae5-4239-bdd2-951929b2aa69","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-07-02T06:10:50.062949Z","iopub.execute_input":"2025-07-02T06:10:50.063189Z","iopub.status.idle":"2025-07-02T06:14:31.873684Z","shell.execute_reply.started":"2025-07-02T06:10:50.063164Z","shell.execute_reply":"2025-07-02T06:14:31.872828Z"},"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting vllm\n  Downloading vllm-0.9.1-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nCollecting torch\n  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from vllm) (2024.11.6)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (7.0.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.4)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\nCollecting blake3 (from vllm)\n  Downloading blake3-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\nRequirement already satisfied: transformers>=4.51.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.51.3)\nCollecting huggingface-hub>=0.32.0 (from huggingface-hub[hf_xet]>=0.32.0->vllm)\n  Downloading huggingface_hub-0.33.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (3.20.3)\nCollecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n  Downloading fastapi-0.115.14-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.18)\nRequirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.70.0)\nRequirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.4)\nRequirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.1.0)\nCollecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\nCollecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\nCollecting llguidance<0.8.0,>=0.7.11 (from vllm)\n  Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nCollecting outlines==0.1.11 (from vllm)\n  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\nCollecting lark==1.2.2 (from vllm)\n  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\nCollecting xgrammar==0.1.19 (from vllm)\n  Downloading xgrammar-0.1.19-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nRequirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.13.2)\nRequirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.18.0)\nCollecting partial-json-parser (from vllm)\n  Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl.metadata (6.1 kB)\nCollecting pyzmq>=25.0.0 (from vllm)\n  Downloading pyzmq-27.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.0 kB)\nCollecting msgspec (from vllm)\n  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nCollecting gguf>=0.13.0 (from vllm)\n  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\nCollecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n  Downloading mistral_common-1.6.3-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\nCollecting compressed-tensors==0.10.1 (from vllm)\n  Downloading compressed_tensors-0.10.1-py3-none-any.whl.metadata (7.0 kB)\nCollecting depyf==0.18.0 (from vllm)\n  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\nCollecting watchfiles (from vllm)\n  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm) (3.3.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.15.2)\nRequirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm) (1.11.1.4)\nRequirement already satisfied: opentelemetry-sdk>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.31.1)\nRequirement already satisfied: opentelemetry-api>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.31.1)\nCollecting opentelemetry-exporter-otlp>=1.26.0 (from vllm)\n  Downloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-semantic-conventions-ai>=0.4.1 (from vllm)\n  Downloading opentelemetry_semantic_conventions_ai-0.4.9-py3-none-any.whl.metadata (1.1 kB)\nCollecting numba==0.61.2 (from vllm)\n  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\nRequirement already satisfied: ray!=2.44.*,>=2.43.0 in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (2.46.0)\nCollecting torch\n  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\nCollecting torchaudio==2.7.0 (from vllm)\n  Downloading torchaudio-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\nCollecting torchvision==0.22.0 (from vllm)\n  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nCollecting xformers==0.0.30 (from vllm)\n  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nCollecting sympy>=1.13.3 (from torch)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nCollecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.7.77 (from torch)\n  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.26.2 (from torch)\n  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvtx-cu12==12.6.77 (from torch)\n  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting triton==3.3.0 (from torch)\n  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nCollecting astor (from depyf==0.18.0->vllm)\n  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.3.8)\nCollecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\nCollecting interegular (from outlines==0.1.11->vllm)\n  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\nCollecting diskcache (from outlines==0.1.11->vllm)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\nRequirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\nCollecting pycountry (from outlines==0.1.11->vllm)\n  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\nCollecting airportsdata (from outlines==0.1.11->vllm)\n  Downloading airportsdata-20250622-py3-none-any.whl.metadata (9.1 kB)\nCollecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n  Downloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\nCollecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\nCollecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\nCollecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\nCollecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.32.0->huggingface-hub[hf_xet]>=0.32.0->vllm) (25.0)\nCollecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.32.0->huggingface-hub[hf_xet]>=0.32.0->vllm)\n  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (2.4.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (4.9.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.9.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.26.0->vllm) (1.2.18)\nCollecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.26.0->vllm)\n  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\nCollecting opentelemetry-exporter-otlp-proto-grpc==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->vllm)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-exporter-otlp-proto-http==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->vllm)\n  Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm) (1.70.0)\nRequirement already satisfied: grpcio<2.0.0,>=1.63.2 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm) (1.72.0rc1)\nCollecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-sdk>=1.26.0 (from vllm)\n  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\nCollecting protobuf (from vllm)\n  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\nCollecting opentelemetry-api>=1.26.0 (from vllm)\n  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk>=1.26.0->vllm)\n  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.26.0->vllm) (8.7.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->vllm) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->vllm) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->vllm) (0.4.0)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.1.8)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\nRequirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (13.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2025.4.26)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.1->vllm) (0.5.3)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.20.0)\nRequirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\nRequirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.2)\nCollecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading rich_toolkit-0.14.8-py3-none-any.whl.metadata (999 bytes)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.14.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.26.0->vllm) (3.21.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2024.10.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.24.0)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nCollecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vllm) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vllm) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->vllm) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->vllm) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->vllm) (2024.2.0)\nRequirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.0.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\nRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.26.0->vllm) (1.17.2)\nDownloading vllm-0.9.1-cp38-abi3-manylinux1_x86_64.whl (394.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.6/394.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading compressed_tensors-0.10.1-py3-none-any.whl (116 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m52.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\nDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m116.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchaudio-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl (31.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading xgrammar-0.1.19-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.14-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.5/95.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gguf-0.17.1-py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.4/515.4 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mistral_common-1.6.3-py3-none-any.whl (6.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl (7.0 kB)\nDownloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\nDownloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl (17 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\nDownloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions_ai-0.4.9-py3-none-any.whl (5.6 kB)\nDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\nDownloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyzmq-27.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (856 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.6/856.6 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading blake3-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.5/385.5 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl (10 kB)\nDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\nDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\nDownloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading airportsdata-20250622-py3-none-any.whl (912 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.7/912.7 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nDownloading rich_toolkit-0.14.8-py3-none-any.whl (24 kB)\nDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, blake3, uvloop, uvicorn, triton, sympy, pyzmq, python-multipart, python-dotenv, pycountry, protobuf, partial-json-parser, opentelemetry-semantic-conventions-ai, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, llvmlite, llguidance, lark, interegular, httptools, hf-xet, diskcache, astor, airportsdata, watchfiles, starlette, opentelemetry-proto, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface-hub, depyf, rich-toolkit, prometheus-fastapi-instrumentator, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, nvidia-cusolver-cu12, lm-format-enforcer, fastapi, torch, outlines_core, opentelemetry-sdk, fastapi-cli, torchaudio, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-exporter-otlp, mistral_common, xgrammar, xformers, torchvision, outlines, numba, gguf, compressed-tensors, vllm\n  Attempting uninstall: nvidia-cusparselt-cu12\n    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: pyzmq\n    Found existing installation: pyzmq 24.0.1\n    Uninstalling pyzmq-24.0.1:\n      Successfully uninstalled pyzmq-24.0.1\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: llvmlite\n    Found existing installation: llvmlite 0.43.0\n    Uninstalling llvmlite-0.43.0:\n      Successfully uninstalled llvmlite-0.43.0\n  Attempting uninstall: hf-xet\n    Found existing installation: hf-xet 1.1.0\n    Uninstalling hf-xet-1.1.0:\n      Successfully uninstalled hf-xet-1.1.0\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.31.1\n    Uninstalling opentelemetry-api-1.31.1:\n      Successfully uninstalled opentelemetry-api-1.31.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.31.1\n    Uninstalling huggingface-hub-0.31.1:\n      Successfully uninstalled huggingface-hub-0.31.1\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.52b1\n    Uninstalling opentelemetry-semantic-conventions-0.52b1:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.52b1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.31.1\n    Uninstalling opentelemetry-sdk-1.31.1:\n      Successfully uninstalled opentelemetry-sdk-1.31.1\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.6.0+cu124\n    Uninstalling torchaudio-2.6.0+cu124:\n      Successfully uninstalled torchaudio-2.6.0+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\n  Attempting uninstall: numba\n    Found existing installation: numba 0.60.0\n    Uninstalling numba-0.60.0:\n      Successfully uninstalled numba-0.60.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ncuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ncudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ndistributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\nydata-profiling 4.16.1 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed airportsdata-20250622 astor-0.8.1 blake3-1.0.5 compressed-tensors-0.10.1 depyf-0.18.0 diskcache-5.6.3 fastapi-0.115.14 fastapi-cli-0.0.7 gguf-0.17.1 hf-xet-1.1.5 httptools-0.6.4 huggingface-hub-0.33.1 interegular-0.3.3 lark-1.2.2 llguidance-0.7.30 llvmlite-0.44.0 lm-format-enforcer-0.10.11 mistral_common-1.6.3 msgspec-0.19.0 numba-0.61.2 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-exporter-otlp-proto-http-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 opentelemetry-semantic-conventions-ai-0.4.9 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post6 prometheus-fastapi-instrumentator-7.1.0 protobuf-5.29.5 pycountry-24.6.1 python-dotenv-1.1.1 python-multipart-0.0.20 pyzmq-27.0.0 rich-toolkit-0.14.8 starlette-0.46.2 sympy-1.14.0 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0 triton-3.3.0 uvicorn-0.35.0 uvloop-0.21.0 vllm-0.9.1 watchfiles-1.1.0 xformers-0.0.30 xgrammar-0.1.19\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# vllm server running in python subprocess","metadata":{"_uuid":"4e5e6edc-e1b7-4a1c-a7b3-be2ef9b19791","_cell_guid":"489eb5be-5ffd-4739-b781-8c2f82d0ad89","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# \nimport subprocess\n# import atexit\nimport os\n\ncmd = [\n    \"vllm\", \"serve\", \"Menlo/Jan-nano-128k\",\n    \"--host\", \"0.0.0.0\",\n    \"--port\", \"1234\",\n    \"--enable-auto-tool-choice\",\n    \"--tool-call-parser\", \"hermes\",\n    \"--rope-scaling\", '{\"rope_type\":\"yarn\",\"factor\":3.2,\"original_max_position_embeddings\":40960}',\n    \"--max-model-len\", \"32768\",\n    \"--tensor-parallel-size\", \"2\",\n    \"--gpu-memory-utilization\", \"0.8\"\n]\n\n# cmd = [\n#       \"vllm\" ,\"serve\", \"google/gemma-3n-e4b-it\" ,\n#       \"--host\" ,\"0.0.0.0\" ,\n#       \"--port\",\"1234\" ,\n#       \"--tensor-parallel-size\", \"2\" ,\n#       \"--dtype\" ,\"half\",\n#       \"--max-model-len\" ,\"32768\" ,\n#       \"--gpu-memory-utilization\",\"0.95\"\n\n# ]\n\n\n\nprocess = subprocess.Popen(\n    cmd,\n    stdout=open(\"jan128k.log\", \"w\"),\n    stderr=subprocess.STDOUT\n)\n\nprint(f\"Started vLLM with PID {process.pid}\")\n","metadata":{"_uuid":"c52e19e2-dd37-469e-abe7-974dc3cc170d","_cell_guid":"bbcb8cdc-b826-4d7c-93d3-fb7978a84103","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T06:26:30.906538Z","iopub.execute_input":"2025-07-02T06:26:30.907286Z","iopub.status.idle":"2025-07-02T06:26:30.915133Z","shell.execute_reply.started":"2025-07-02T06:26:30.907253Z","shell.execute_reply":"2025-07-02T06:26:30.914267Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Started vLLM with PID 924\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"### vllm logs","metadata":{"_uuid":"3ce68c75-9181-4843-87c3-3063b8423aa5","_cell_guid":"fde0aa7a-c92e-48db-8257-698658145257","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# \n!ps -ax | grep vllm\n!tail -n 200 jan128k.log","metadata":{"_uuid":"0e255386-1768-41cd-96eb-158d0f50ac16","_cell_guid":"9316daca-9749-4bda-9f1b-02bc8f2cf446","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T06:50:26.296612Z","iopub.execute_input":"2025-07-02T06:50:26.297080Z","iopub.status.idle":"2025-07-02T06:50:26.541354Z","shell.execute_reply.started":"2025-07-02T06:50:26.297049Z","shell.execute_reply":"2025-07-02T06:50:26.540704Z"},"scrolled":true,"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"    924 ?        Sl     0:21 /usr/bin/python3 /usr/local/bin/vllm serve Menlo/Jan-nano-128k --host 0\n   1097 pts/0    Ss+    0:00 /bin/bash -c ps -ax | grep vllm\n   1099 pts/0    S+     0:00 grep vllm\nINFO 07-02 06:26:36 [__init__.py:244] Automatically detected platform cuda.\n2025-07-02 06:26:37.469266: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751437597.493021     924 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751437597.500211     924 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nINFO 07-02 06:26:48 [api_server.py:1287] vLLM API server version 0.9.1\nINFO 07-02 06:26:50 [cli_args.py:309] non-default args: {'host': '0.0.0.0', 'port': 1234, 'enable_auto_tool_choice': True, 'tool_call_parser': 'hermes', 'model': 'Menlo/Jan-nano-128k', 'rope_scaling': {'rope_type': 'yarn', 'factor': 3.2, 'original_max_position_embeddings': 40960}, 'max_model_len': 32768, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.8}\nINFO 07-02 06:26:50 [config.py:533] Overriding HF config with {'rope_scaling': {'rope_type': 'yarn', 'factor': 3.2, 'original_max_position_embeddings': 40960}}\nINFO 07-02 06:27:06 [config.py:823] This model supports multiple tasks: {'generate', 'embed', 'score', 'reward', 'classify'}. Defaulting to 'generate'.\nWARNING 07-02 06:27:06 [config.py:3220] Your device 'Tesla T4' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\nWARNING 07-02 06:27:06 [config.py:3271] Casting torch.bfloat16 to torch.float16.\nWARNING 07-02 06:27:06 [arg_utils.py:1642] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \nINFO 07-02 06:27:06 [config.py:1946] Defaulting to use mp for distributed inference\nINFO 07-02 06:27:06 [api_server.py:265] Started engine process with PID 962\nWARNING 07-02 06:27:07 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\nINFO 07-02 06:27:11 [__init__.py:244] Automatically detected platform cuda.\n2025-07-02 06:27:12.045769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751437632.068924     962 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751437632.075936     962 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nINFO 07-02 06:27:19 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1) with config: model='Menlo/Jan-nano-128k', speculative_config=None, tokenizer='Menlo/Jan-nano-128k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Menlo/Jan-nano-128k, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=True, \nWARNING 07-02 06:27:20 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 07-02 06:27:20 [cuda.py:275] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 07-02 06:27:20 [cuda.py:324] Using XFormers backend.\nWARNING 07-02 06:27:21 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\nINFO 07-02 06:27:25 [__init__.py:244] Automatically detected platform cuda.\n2025-07-02 06:27:26.109998: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751437646.133075     981 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751437646.139892     981 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:27:33 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:27:33 [cuda.py:275] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:27:33 [cuda.py:324] Using XFormers backend.\n[W702 06:27:44.747291562 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W702 06:27:44.130905647 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W702 06:27:54.757879050 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W702 06:28:04.768337564 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:28:04 [utils.py:1126] Found nccl from library libnccl.so.2\nINFO 07-02 06:28:04 [utils.py:1126] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:28:04 [pynccl.py:70] vLLM is using nccl==2.26.2\nINFO 07-02 06:28:04 [pynccl.py:70] vLLM is using nccl==2.26.2\nINFO 07-02 06:28:04 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:28:04 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\nINFO 07-02 06:28:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_202ac59f'), local_subscribe_addr='ipc:///tmp/74d23c2a-6c11-4bf7-8997-38b7853e4938', remote_subscribe_addr=None, remote_addr_ipv6=False)\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:28:04 [parallel_state.py:1065] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\nINFO 07-02 06:28:04 [parallel_state.py:1065] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nINFO 07-02 06:28:04 [model_runner.py:1171] Starting to load model Menlo/Jan-nano-128k...\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:28:04 [model_runner.py:1171] Starting to load model Menlo/Jan-nano-128k...\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:28:05 [weight_utils.py:292] Using model weights format ['*.safetensors']\nINFO 07-02 06:28:05 [weight_utils.py:292] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:11<00:11, 11.41s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:15<00:00,  6.87s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:15<00:00,  7.55s/it]\n\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:28:21 [default_loader.py:272] Loading weights took 15.38 seconds\nINFO 07-02 06:28:21 [default_loader.py:272] Loading weights took 15.26 seconds\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:28:22 [model_runner.py:1203] Model loading took 3.8364 GiB and 16.067721 seconds\nINFO 07-02 06:28:22 [model_runner.py:1203] Model loading took 3.8364 GiB and 16.070561 seconds\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:28:32 [worker.py:294] Memory profiling takes 9.85 seconds\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:28:32 [worker.py:294] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.80) = 11.79GiB\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:28:32 [worker.py:294] model weights take 3.84GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.68GiB; the rest of the memory reserved for KV Cache is 6.15GiB.\nINFO 07-02 06:28:32 [worker.py:294] Memory profiling takes 9.96 seconds\nINFO 07-02 06:28:32 [worker.py:294] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.80) = 11.79GiB\nINFO 07-02 06:28:32 [worker.py:294] model weights take 3.84GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.68GiB; the rest of the memory reserved for KV Cache is 6.15GiB.\nINFO 07-02 06:28:33 [executor_base.py:113] # cuda blocks: 5598, # CPU blocks: 3640\nINFO 07-02 06:28:33 [executor_base.py:118] Maximum concurrency for 32768 tokens per request: 2.73x\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:28:37 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 07-02 06:28:37 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nCapturing CUDA graph shapes: 100%|██████████| 35/35 [00:36<00:00,  1.04s/it]\nINFO 07-02 06:29:13 [custom_all_reduce.py:196] Registering 2555 cuda graph addresses\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:29:14 [custom_all_reduce.py:196] Registering 2555 cuda graph addresses\n\u001b[1;36m(VllmWorkerProcess pid=981)\u001b[0;0m INFO 07-02 06:29:14 [model_runner.py:1671] Graph capturing finished in 37 secs, took 0.26 GiB\nINFO 07-02 06:29:14 [model_runner.py:1671] Graph capturing finished in 37 secs, took 0.26 GiB\nINFO 07-02 06:29:14 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 52.05 seconds\nINFO 07-02 06:29:14 [serving_chat.py:81] \"auto\" tool choice has been enabled please note that while the parallel_tool_calls client option is preset for compatibility reasons, it will be ignored.\nWARNING 07-02 06:29:14 [logger.py:65] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\nINFO 07-02 06:29:14 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8, 'min_p': 0.0}\nINFO 07-02 06:29:14 [serving_completion.py:66] Using default completion sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8, 'min_p': 0.0}\nINFO 07-02 06:29:14 [api_server.py:1349] Starting vLLM API server 0 on http://0.0.0.0:1234\nINFO 07-02 06:29:14 [launcher.py:29] Available routes are:\nINFO 07-02 06:29:14 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET\nINFO 07-02 06:29:14 [launcher.py:37] Route: /docs, Methods: HEAD, GET\nINFO 07-02 06:29:14 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET\nINFO 07-02 06:29:14 [launcher.py:37] Route: /redoc, Methods: HEAD, GET\nINFO 07-02 06:29:14 [launcher.py:37] Route: /health, Methods: GET\nINFO 07-02 06:29:14 [launcher.py:37] Route: /load, Methods: GET\nINFO 07-02 06:29:14 [launcher.py:37] Route: /ping, Methods: POST\nINFO 07-02 06:29:14 [launcher.py:37] Route: /ping, Methods: GET\nINFO 07-02 06:29:14 [launcher.py:37] Route: /tokenize, Methods: POST\nINFO 07-02 06:29:14 [launcher.py:37] Route: /detokenize, Methods: POST\nINFO 07-02 06:29:14 [launcher.py:37] Route: /v1/models, Methods: GET\nINFO 07-02 06:29:14 [launcher.py:37] Route: /version, Methods: GET\nINFO 07-02 06:29:14 [launcher.py:37] Route: /v1/chat/completions, Methods: POST\nINFO 07-02 06:29:14 [launcher.py:37] Route: /v1/completions, Methods: POST\nINFO 07-02 06:29:14 [launcher.py:37] Route: /v1/embeddings, Methods: POST\nINFO 07-02 06:29:14 [launcher.py:37] Route: /pooling, Methods: POST\nINFO 07-02 06:29:14 [launcher.py:37] Route: /classify, Methods: POST\nINFO 07-02 06:29:14 [launcher.py:37] Route: /score, Methods: POST\nINFO 07-02 06:29:14 [launcher.py:37] Route: /v1/score, Methods: POST\nINFO 07-02 06:29:14 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST\nINFO 07-02 06:29:14 [launcher.py:37] Route: /rerank, Methods: POST\nINFO 07-02 06:29:14 [launcher.py:37] Route: /v1/rerank, Methods: POST\nINFO 07-02 06:29:14 [launcher.py:37] Route: /v2/rerank, Methods: POST\nINFO 07-02 06:29:14 [launcher.py:37] Route: /invocations, Methods: POST\nINFO 07-02 06:29:14 [launcher.py:37] Route: /metrics, Methods: GET\nINFO:     Started server process [924]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# \n# Check logs\n# import time\n# time.sleep(30)  # Wait for startup\n# !tail -n 50 jan128k.log\n\n# Verify server is running\n!ps -ax | grep vllm\n!netstat -tulnp | grep 1234","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:50:32.804318Z","iopub.execute_input":"2025-07-02T06:50:32.805232Z","iopub.status.idle":"2025-07-02T06:50:33.058625Z","shell.execute_reply.started":"2025-07-02T06:50:32.805194Z","shell.execute_reply":"2025-07-02T06:50:33.057626Z"}},"outputs":[{"name":"stdout","text":"    924 ?        Sl     0:21 /usr/bin/python3 /usr/local/bin/vllm serve Menlo/Jan-nano-128k --host 0\n   1101 pts/0    Ss+    0:00 /bin/bash -c ps -ax | grep vllm\n   1103 pts/0    S+     0:00 grep vllm\ntcp        0      0 0.0.0.0:1234            0.0.0.0:*               LISTEN      924/python3         \n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## Ngrok setup","metadata":{"_uuid":"cce11d5d-0418-4ae6-a456-d6d86642f512","_cell_guid":"9f1ab0bf-ac09-49ad-8f93-5792121a9cf2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# \n# Ngrok setup\n!pip install pyngrok --quiet\n","metadata":{"_uuid":"4339a4e1-881d-484b-8213-33ca0275faf3","_cell_guid":"64481632-d853-49b7-a244-f16bc6a1b538","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T06:50:35.364561Z","iopub.execute_input":"2025-07-02T06:50:35.365451Z","iopub.status.idle":"2025-07-02T06:50:38.861993Z","shell.execute_reply.started":"2025-07-02T06:50:35.365386Z","shell.execute_reply":"2025-07-02T06:50:38.861178Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# \nfrom pyngrok import ngrok\n\n# Replace with your actual ngrok authtoken\nngrok.set_auth_token(\"2rqzh1D81plieczVNZ0A2g6Xzrh_2aVgQvijc8brnWUaHd8AF\")  # 🛡️ Required for authenticated access :contentReference[oaicite:3]{index=3}\n\ntunnel = ngrok.connect(1234)\nprint(\"Public URL:\", tunnel.public_url)","metadata":{"_uuid":"9044713e-fb31-4838-9841-9edca8c97d37","_cell_guid":"c2d14bbc-7159-4f8a-86ea-7c96c116f693","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-02T06:50:38.863504Z","iopub.execute_input":"2025-07-02T06:50:38.863815Z","iopub.status.idle":"2025-07-02T06:50:38.950212Z","shell.execute_reply.started":"2025-07-02T06:50:38.863789Z","shell.execute_reply":"2025-07-02T06:50:38.949622Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Public URL: https://4389-34-10-227-245.ngrok-free.app\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import requests\nimport json\n\ntry:\n    response = requests.get(f\"{tunnel.public_url}/health\", timeout=10)\n    print(\"Server health:\", response.status_code)\nexcept Exception as e:\n    print(\"Server not ready:\", e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:50:39.384418Z","iopub.execute_input":"2025-07-02T06:50:39.384770Z","iopub.status.idle":"2025-07-02T06:50:39.518324Z","shell.execute_reply.started":"2025-07-02T06:50:39.384719Z","shell.execute_reply":"2025-07-02T06:50:39.517605Z"}},"outputs":[{"name":"stdout","text":"Server health: 200\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"### List and Kill ngrok","metadata":{"_uuid":"351ac7dd-f3cc-4eb5-986a-0d8b2ac7c9db","_cell_guid":"b8c3e555-ea0a-485b-8dea-ad361b82761d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!ps -ax | grep ngrok\n!killall ngrok","metadata":{"_uuid":"971f1a36-c4d9-4054-b50c-76af89492b15","_cell_guid":"065bd5cc-9b5b-4784-ad5b-2c18e03f5cf5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-02T06:57:13.214439Z","iopub.execute_input":"2025-07-02T06:57:13.214757Z","iopub.status.idle":"2025-07-02T06:57:13.452575Z","shell.execute_reply.started":"2025-07-02T06:57:13.214736Z","shell.execute_reply":"2025-07-02T06:57:13.451616Z"}},"outputs":[{"name":"stdout","text":"    891 ?        Sl     0:05 /root/.config/ngrok/ngrok start --none --log stdout\n   1124 pts/0    Ss+    0:00 /bin/bash -c ps -ax | grep ngrok\n   1126 pts/0    S+     0:00 grep ngrok\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"## Tunnelmole","metadata":{}},{"cell_type":"code","source":"# \n!curl -O https://install.tunnelmole.com/t357g/install && sudo bash install\n","metadata":{"_uuid":"3a551b48-0794-43e0-a20b-22e6428a97de","_cell_guid":"f78ac49f-b841-4dd0-a8d9-d8be20c18989","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-02T06:59:12.225100Z","iopub.execute_input":"2025-07-02T06:59:12.225417Z","iopub.status.idle":"2025-07-02T06:59:14.551908Z","shell.execute_reply.started":"2025-07-02T06:59:12.225389Z","shell.execute_reply":"2025-07-02T06:59:14.551214Z"}},"outputs":[{"name":"stdout","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   721    0   721    0     0   3130      0 --:--:-- --:--:-- --:--:--  3134\nInstalling Tunnelmole for Linux\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  1427  100  1427    0     0   3249      0 --:--:-- --:--:-- --:--:--  3243\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 17.4M  100 17.4M    0     0  19.2M      0 --:--:-- --:--:-- --:--:-- 19.2M\n  _______                     _                 _      \n |__   __|                   | |               | |     \n    | |_   _ _ __  _ __   ___| |_ __ ___   ___ | | ___ \n    | | | | | '_ \\| '_ \\ / _ \\ | '_ ` _ \\ / _ \\| |/ _ \\ \n    | | |_| | | | | | | |  __/ | | | | | | (_) | |  __/\n    |_|\\__,_|_| |_|_| |_|\\___|_|_| |_| |_|\\___/|_|\\___|\n                                                       \n                                                       \n\nCongrats! tunnelmole is now installed 😃\nNow what?\n - Get a random public URL for a local server: \"tmole <port>\" e.g. \"tmole 80\" if your server is running on port 80\n - Get a customized public URL for a local server: \"tmole 80 as mysite.tunnelmole.com\"\n - Read the docs for more detailed instructions https://tunnelmole.com/docs\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import subprocess\n\nprocess = subprocess.Popen(\n    [\"tmole\", \"1234\"],\n    stdout=open(\"tunnelmole.log\", \"w\"),\n    stderr=subprocess.STDOUT\n)\nprint(\"Tunnelmole started with PID:\", process.pid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:59:20.424921Z","iopub.execute_input":"2025-07-02T06:59:20.425222Z","iopub.status.idle":"2025-07-02T06:59:20.432314Z","shell.execute_reply.started":"2025-07-02T06:59:20.425195Z","shell.execute_reply":"2025-07-02T06:59:20.431547Z"}},"outputs":[{"name":"stdout","text":"Tunnelmole started with PID: 1147\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"!tail -n 20 tunnelmole.log","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:59:26.295036Z","iopub.execute_input":"2025-07-02T06:59:26.295783Z","iopub.status.idle":"2025-07-02T06:59:26.414253Z","shell.execute_reply.started":"2025-07-02T06:59:26.295760Z","shell.execute_reply":"2025-07-02T06:59:26.413587Z"}},"outputs":[{"name":"stdout","text":"https://f8kfix-ip-34-10-227-245.tunnelmole.net ⟶   http://localhost:1234\nhttp://f8kfix-ip-34-10-227-245.tunnelmole.net ⟶   http://localhost:1234\n\n\nShare to Reddit: https://dashboard.tunnelmole.com/share/reddit/aHR0cHM6Ly9mOGtmaXgtaXAtMzQtMTAtMjI3LTI0NS50dW5uZWxtb2xlLm5ldA==\nShare to X/Twitter: https://dashboard.tunnelmole.com/share/twitter/aHR0cHM6Ly9mOGtmaXgtaXAtMzQtMTAtMjI3LTI0NS50dW5uZWxtb2xlLm5ldA==\nShare to Hacker News: https://dashboard.tunnelmole.com/share/hackernews/aHR0cHM6Ly9mOGtmaXgtaXAtMzQtMTAtMjI3LTI0NS50dW5uZWxtb2xlLm5ldA==\n\n\n\n\n\n\nHey there 👋. I'm Robbie, the creator of Tunnelmole. I'm looking to improve Tunnelmole and your feedback could make a huge difference!\n\nWould you be interested in participating in a short interview over video call? As thanks, I'm offering a free 3-month Tunnelmole subscription. Send me an email at robbie-cahill@proton.me and we can set something up.\n\nThanks for supporting the community!\n\n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}